{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Transfer Learning with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yryz3lhx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-yryz3lhx\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/nick/.local/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /home/nick/.local/lib/python3.10/site-packages (from clip==1.0) (2024.4.28)\n",
      "Requirement already satisfied: torch in /home/nick/.local/lib/python3.10/site-packages (from clip==1.0) (2.3.0+cu118)\n",
      "Requirement already satisfied: torchvision in /home/nick/.local/lib/python3.10/site-packages (from clip==1.0) (0.18.0+cu118)\n",
      "Requirement already satisfied: tqdm in /home/nick/.local/lib/python3.10/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/nick/.local/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: fsspec in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.8.86)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.0.86)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (2.3.0)\n",
      "Requirement already satisfied: sympy in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: filelock in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (2.20.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nick/.local/lib/python3.10/site-packages (from torch->clip==1.0) (4.9.0)\n",
      "Requirement already satisfied: numpy in /home/nick/.local/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/nick/.local/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nick/.local/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nick/.local/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 data\n",
    "cifar10 = CIFAR10(root=\"data\", train=True, download=True)\n",
    "cifar10_classes = cifar10.classes\n",
    "\n",
    "# Small subset of CIFAR-10 images\n",
    "subset_size = 10\n",
    "images = [cifar10[i][0] for i in range(subset_size)]\n",
    "class_labels = [cifar10_classes[cifar10[i][1]] for i in range(subset_size)]\n",
    "\n",
    "# Preprocess images\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),  # Resize image to fit CLIP input dimensions\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "images = torch.stack([transform(image).to(device) for image in images])\n",
    "\n",
    "# Prepare the text inputs\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {label}\") for label in class_labels]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted labels:\n",
      "Image 1 (True label: frog): airplane\n",
      "Image 2 (True label: truck): automobile\n",
      "Image 3 (True label: truck): automobile\n",
      "Image 4 (True label: deer): cat\n",
      "Image 5 (True label: automobile): deer\n",
      "Image 6 (True label: automobile): deer\n",
      "Image 7 (True label: bird): frog\n",
      "Image 8 (True label: horse): horse\n",
      "Image 9 (True label: ship): ship\n",
      "Image 10 (True label: cat): truck\n"
     ]
    }
   ],
   "source": [
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Normalize features to unit length\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "logits_per_image = image_features @ text_features.T\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "# Display the top-1 predicted class for each image\n",
    "print(\"Top predicted labels:\")\n",
    "for i, prob in enumerate(probs):\n",
    "    top_class = cifar10_classes[prob.argmax().item()]\n",
    "    print(f\"Image {i + 1} (True label: {class_labels[i]}): {top_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MobilenetV2 instead of Bigtransfer; Bigtransfer used too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nickk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = tf.image.resize(x_train, (96, 96))  # Resize images to match MobileNetV2 input size\n",
    "x_test = tf.image.resize(x_test, (96, 96))\n",
    "x_train = tf.keras.applications.mobilenet_v2.preprocess_input(x_train)\n",
    "x_test = tf.keras.applications.mobilenet_v2.preprocess_input(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nickk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nickk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2 model, pre-trained on ImageNet\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 84s 27ms/step - loss: 0.4575 - accuracy: 0.8431 - val_loss: 0.4466 - val_accuracy: 0.8496\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 80s 26ms/step - loss: 0.3516 - accuracy: 0.8771 - val_loss: 0.4278 - val_accuracy: 0.8584\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 85s 27ms/step - loss: 0.2816 - accuracy: 0.9015 - val_loss: 0.4213 - val_accuracy: 0.8648\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 79s 25ms/step - loss: 0.2299 - accuracy: 0.9197 - val_loss: 0.4829 - val_accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 79s 25ms/step - loss: 0.1785 - accuracy: 0.9372 - val_loss: 0.5276 - val_accuracy: 0.8623\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.5276 - accuracy: 0.8623\n",
      "Test accuracy: 0.8622999787330627\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
